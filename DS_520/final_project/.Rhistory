n.x = 8,
mean.y = NULL,
sigma.y = NULL,
n.y = NULL,
alternative = "less",
mu = 18.5,
conf.level = 0.95
)
x = c(3,5,6,9,4,3)
y = c(7,8,7,9,8,9)
z.test(
x = x,
y = y,
alternative = "less",
mu = 0,
sigma.x = sd(x),
sigma.y = sd(y),
conf.level = 0.95
)
t.test(x = x, y = y,
alternative = c("less"),
mu = 0, paired = FALSE, var.equal = FALSE,
conf.level = 0.95, ...)
t.test(x = x, y = y,
alternative = c("less"),
mu = 0, paired = FALSE, var.equal = FALSE,
conf.level = 0.95)
mean(x) - mean(y)
var.test(x,y)
t.test(x = x, y = y,
alternative = c("less"),
mu = 0, paired = FALSE, var.equal = T,
conf.level = 0.95)
t.test(x = x, y = y,
alternative = c("less"),
mu = 0, paired = FALSE, var.equal = FALSE,
conf.level = 0.95)
t.test(x = x, y = y,
alternative = c("less"),
mu = 0, paired = T, var.equal = FALSE,
conf.level = 0.95)
z.test(
x = x,
y = y,
alternative = "less",
mu = 0,
sigma.x = sd(x),
sigma.y = sd(y),
conf.level = 0.95
)
t.test(x = x, y = y,
alternative = c("less"),
mu = 0, paired = F, var.equal = FALSE,
conf.level = 0.95)
t.test(x = x, y = y,
alternative = c("less"),
mu = 0, paired = F, var.equal = T,
conf.level = 0.95)
t.test(x = x, y = y,
alternative = c("less"),
mu = 0, paired = F, var.equal = F,
conf.level = 0.95)
t.test(x = x, y = y,
alternative = c("less"),
mu = 0, paired = T, var.equal = F,
conf.level = 0.95)
t.test(x = x, y = y,
alternative = c("less"),
mu = 0, paired = F, var.equal = F,
conf.level = 0.95)
t.test(x = x, y = y,
alternative = c("less"),
mu = 0, paired = F, var.equal = T,
conf.level = 0.95)
t.test(x = x, y = y,
alternative = c("less"),
mu = 0, paired = T, var.equal = T,
conf.level = 0.95)
t.test(x = x, y = y,
alternative = c("less"),
mu = 0, paired = F, var.equal = T,
conf.level = 0.95)
my_data = data.frame(name = paste0(rep("mice_", 10),1:10), weight = round(rnorm(10,20,2),1))
my_data
t,test(my_data$weight, mu =25)
t.test(my_data$weight, mu =25)
set.seed(1234)
my_data = data.frame(name = paste0(rep("mice_", 10),1:10), weight = round(rnorm(10,20,2),1))
my_data
t.test(my_data$weight, mu =25)
set.seed(1234)
my_data = data.frame(name = paste0(rep("mice_", 10),1:10), weight = round(rnorm(10,20,2),1))
my_data
t.test(my_data$weight, mu =25)
# set.seed(1234)
my_data = data.frame(name = paste0(rep("mice_", 10),1:10), weight = round(rnorm(10,20,2),1))
my_data
t.test(my_data$weight, mu =25)
# set.seed(1234)
my_data = data.frame(name = paste0(rep("mice_", 10),1:10), weight = round(rnorm(10,20,2),1))
my_data
t.test(my_data$weight, mu =25)
t.test(my_data$weight, mu =25)
# set.seed(1234)
my_data = data.frame(name = paste0(rep("mice_", 10),1:10), weight = round(rnorm(10,20,2),1))
my_data
t.test(my_data$weight, mu =25)
t.test(my_data$weight, mu =25, alternative = "less")
t.test(my_data$weight, mu =25, alternative = "greater")
t.test(my_data$weight, mu =25, alternative = "two.sided")
qnorm(z)
x = 12.170789
m = 2
s = 8.276280
n = 9626
z = (x-m)/(s/sqrt(n))
qnorm(z)
x = 12.170789
m = 2
s = 8.276280
n = 9626
z = (x-m)/(s/sqrt(n))
z
z = (x-m)/(s)
x = 12.170789
m = 2
s = 8.276280
n = 9626
z = (x-m)/(s)
z
pnorm(z, mean = m, sd = s, lower.tail = F)
pnorm(z, mean = m, sd = 1, lower.tail = F)
pnorm(z, mean = 0, sd = 1, lower.tail = F)
pnorm(z, mean = m, sd = s/n, lower.tail = F)
pnorm(z, mean = m, sd = s, lower.tail = F)
x = 12.170789
m = 2
s = 8.276280
n = 9626
z = (x-m)/s
z
pnorm(z, mean = m, sd = s, lower.tail = F)
pnorm(z, lower.tail = F)
pnorm(z, mean = m, sd = s, lower.tail = F)
pnorm(x, mean = m, sd = s, lower.tail = F)
pnorm(x, mean = m, sd = s/sqrt(n), lower.tail = F)
pnorm(z, mean = m, sd = s/sqrt(n), lower.tail = F)
pnorm(x, mean = m, sd = s/sqrt(n), lower.tail = F)
pnorm(z, mean = m, sd = s/sqrt(n), lower.tail = F)
pnorm(x, mean = m, sd = s/sqrt(n), lower.tail = F)
pnorm(z, mean = m, sd = s/sqrt(n), lower.tail = F)
pnorm(x, mean = m, sd = s/sqrt(n), lower.tail = F)
x = 12.170789
m = 2
s = 8.276280
n = 9626
z = (x-m)/(s/sqrt(n))
z
pnorm(z, mean = m, sd = s/sqrt(n), lower.tail = F)
z
#load necessary libraries
library(dplyr)
library(readxl)
library(ggplot2)
library(ggpubr)
library(qqplotr)
#library(car)
library(e1071)
library(nortest)
library(BSDA)
library(psych)
library(caret)
library(leaps)
library(gvlma)
# options("scipen"=100, "digits"=6)
# Import the csv into a dataframe
file_name = "auto-mpg.csv"
df = read.csv(file_name)
head(df)
# structure of the dataframe
str(df)
# Converting the horsepower column to a numeric column
df$horsepower = as.numeric(df$horsepower)
head(df)
# Summary of the dataframe
summary(df)
# Removing all case with null value in any columns
df <- na.omit(df)
summary(df)
# Scatter plot of matrices
pairs.panels(df[,1:6],method = "pearson",hist.col ="#00AFBB" ,density = TRUE,ellipses = TRUE)
for (i in names(df[,1:6])) {
boxplot(df[,i], names = "names(df[,i])", ylab = i)
}
#Using the rest 300 samples in the dataframe, run a simple linear regression to determine the relationship between mpg and a single variable
df_train <- df[1:300,1:6]
df_test <- df[301:nrow(df),1:6]
# Linear regression plot
ggplot(data=df_train, aes(x=displacement, y=mpg)) +
geom_smooth(method="lm") +
geom_point() +
stat_regline_equation(label.x=300, label.y=40) +
stat_cor(aes(label=..rr.label..), label.x=300, label.y=38)
#performing regression
dis_model <- lm(mpg~displacement, data=df_train)
summary(dis_model)
# mean of residuals
mean(resid(dis_model))
#plot the variable
plot(df_train$mpg~df_train$displacement,main="mpg vs displacement",xlab="displacement",ylab = "mpg")
abline(dis_model,col="red")
#residuals vs. the predictor variable
residual <- dis_model$residuals
plot(df_train$mpg~residual,lwd=3, col="blue",main="mpg vs residual", xlab="residual",ylab = "mpg")
grid(NA, 5, lwd = 2,col = "darkgray")
#absolute value of the residuals vs. the predictor variable
abs_residual <- abs(residual)
plot(df_train$mpg~abs_residual,lwd=3, col="red",main="mpg vs Abs_residual", xlab="abs_residual",ylab = "mpg")
grid(NA, 5, lwd = 2,col = "darkgray")
#histogram of the residuals
hist(residual,prob=T,breaks=20,main="HISTOGRAM OF DISPLACEMENT RESIDUALS",xlab="Residuals")
lines(density(residual),col="red",lwd=3)
plot(dis_model)
# Make predictions and compute the R2, RMSE and MAE
dis_predict <- dis_model %>% predict(df_test)
data.frame( R2 = R2(dis_predict, df_test$mpg),
RMSE = RMSE(dis_predict, df_test$mpg),
MAE = MAE(dis_predict, df_test$mpg))
prediction_error = RMSE(dis_predict, df_test$mpg)/mean(df_test$mpg)
prediction_error
compare_dis = as.data.frame(cbind(df_test$mpg,dis_predict),row=FALSE)
names(compare_dis) = c("observed","dis_predict")
compare_dis
# Linear regression plot
ggplot(data=df_train, aes(x=horsepower, y=mpg)) +
geom_smooth(method="lm") +
geom_point() +
stat_regline_equation(label.x=200, label.y=40) +
stat_cor(aes(label=..rr.label..), label.x=200, label.y=38)
#performing regression
hors_model <- lm(mpg~horsepower, data=df_train)
summary(hors_model)
#plot the variable
plot(df_train$mpg~df_train$horsepower,main="mpg vs horsepower",xlab="horsepower",ylab = "mpg")
abline(hors_model,col="red")
#residuals vs. the predictor variable
residual <- hors_model$residuals
plot(df_train$mpg~residual,lwd=3, col="blue",main="mpg vs residual", xlab="residual",ylab = "mpg")
grid(NA, 5, lwd = 2,col = "darkgray")
#absolute value of the residuals vs. the predictor variable
abs_residual <- abs(residual)
plot(df_train$mpg~abs_residual,lwd=3, col="red",main="mpg vs Abs_residual", xlab="abs_residual",ylab = "mpg")
grid(NA, 5, lwd = 2,col = "darkgray")
#histogram of the residuals
hist(residual,prob=T,breaks=20,main="HISTOGRAM OF HORSEPOWER RESIDUALS",xlab="Residuals")
lines(density(residual),col="red",lwd=3)
plot(hors_model)
# mean of residuals
mean(resid(hors_model))
# Make predictions and compute the R2, RMSE and MAE
hors_predict <- hors_model %>% predict(df_test)
data.frame( R2 = R2(hors_predict, df_test$mpg),
RMSE = RMSE(hors_predict, df_test$mpg),
MAE = MAE(hors_predict, df_test$mpg))
prediction_error = RMSE(hors_predict, df_test$mpg)/mean(df_test$mpg)
prediction_error
compare_hors = as.data.frame(cbind(df_test$mpg,hors_predict),row=FALSE)
names(compare_hors) = c("observed","hors_predict")
compare_hors
# Linear regression plot
ggplot(data=df_train, aes(x=cylinder, y=mpg)) +
geom_smooth(method="lm") +
geom_point() +
stat_regline_equation(label.x=5, label.y=40) +
stat_cor(aes(label=..rr.label..), label.x=5, label.y=38)
#performing regression
cylinder_model <- lm(mpg~cylinder, data=df_train)
summary(cylinder_model)
# mean of residuals
mean(resid(cylinder_model))
#plot the variable
plot(df_train$mpg~df_train$cylinder,main="mpg vs cylinder",xlab="cylinder",ylab = "mpg")
abline(cylinder_model,col="red")
#residuals vs. the predictor variable
residual <- cylinder_model$residuals
plot(df_train$mpg~residual,lwd=3, col="blue",main="mpg vs residual", xlab="residual",ylab = "mpg")
grid(NA, 5, lwd = 2,col = "darkgray")
#absolute value of the residuals vs. the predictor variable
abs_residual <- abs(residual)
plot(df_train$mpg~abs_residual,lwd=3, col="red",main="mpg vs Abs_residual", xlab="abs_residual",ylab = "mpg")
grid(NA, 5, lwd = 2,col = "darkgray")
#histogram of the residuals
hist(residual,prob=T,breaks=20,main="HISTOGRAM OF CYLINDER RESIDUALS",xlab="Residuals")
lines(density(residual),col="red",lwd=3)
plot(cylinder_model)
# Make predictions and compute the R2, RMSE and MAE
cyl_predict <- cylinder_model %>% predict(df_test)
data.frame( R2 = R2(cyl_predict, df_test$mpg),
RMSE = RMSE(cyl_predict, df_test$mpg),
MAE = MAE(cyl_predict, df_test$mpg))
prediction_error = RMSE(cyl_predict, df_test$mpg)/mean(df_test$mpg)
prediction_error
compare_cyl = as.data.frame(cbind(df_test$mpg,cyl_predict),row=FALSE)
names(compare_cyl) = c("observed","cyl_predict")
compare_cyl
# Linear regression plot
ggplot(data=df_train, aes(x=weight, y=mpg)) +
geom_smooth(method="lm") +
geom_point() +
stat_regline_equation(label.x=3000, label.y=40) +
stat_cor(aes(label=..rr.label..), label.x=3000, label.y=38)
#performing regression
weight_model <- lm(mpg~weight, data=df_train)
summary(weight_model)
# mean of residuals
mean(resid(weight_model))
#plot the variable
plot(df_train$mpg~df_train$weight,main="mpg vs weight",xlab="weight",ylab = "mpg")
abline(weight_model,col="red")
#residuals vs. the predictor variable
residual <- weight_model$residuals
plot(df_train$mpg~residual,lwd=3, col="blue",main="mpg vs residual", xlab="residual",ylab = "mpg")
grid(NA, 5, lwd = 2,col = "darkgray")
#absolute value of the residuals vs. the predictor variable
abs_residual <- abs(residual)
plot(df_train$mpg~abs_residual,lwd=3, col="red",main="mpg vs Abs_residual", xlab="abs_residual",ylab = "mpg")
grid(NA, 5, lwd = 2,col = "darkgray")
#histogram of the residuals
hist(residual,prob=T,breaks=20,main="HISTOGRAM OF WEIGHT RESIDUALS",xlab="Residuals")
lines(density(residual),col="red",lwd=3)
plot(weight_model)
# Make predictions and compute the R2, RMSE and MAE
weight_predict <- weight_model %>% predict(df_test)
data.frame( R2 = R2(weight_predict, df_test$mpg),
RMSE = RMSE(weight_predict, df_test$mpg),
MAE = MAE(weight_predict, df_test$mpg))
prediction_error = RMSE(weight_predict, df_test$mpg)/mean(df_test$mpg)
prediction_error
compare_wght = as.data.frame(cbind(df_test$mpg,weight_predict),row=FALSE)
names(compare_wght) = c("observed","weight_predict")
compare_wght
# Linear regression plot
ggplot(data=df_train, aes(x=acceleration, y=mpg)) +
geom_smooth(method="lm") +
geom_point() +
stat_regline_equation(label.x=8, label.y=40) +
stat_cor(aes(label=..rr.label..), label.x=8, label.y=38)
#performing regression
acc_model <- lm(mpg~acceleration, data=df_train)
summary(acc_model)
# mean of residuals
mean(resid(acc_model))
#plot the variable
plot(df_train$mpg~df_train$acceleration,main="mpg vs acceleration",xlab="acceleration",ylab = "mpg")
abline(acc_model,col="red")
#residuals vs. the predictor variable
residual <- acc_model$residuals
plot(df_train$mpg~residual,lwd=3, col="blue",main="mpg vs residual", xlab="residual",ylab = "mpg")
grid(NA, 5, lwd = 2,col = "darkgray")
#absolute value of the residuals vs. the predictor variable
abs_residual <- abs(residual)
plot(df_train$mpg~abs_residual,lwd=3, col="red",main="mpg vs Abs_residual", xlab="abs_residual",ylab = "mpg")
grid(NA, 5, lwd = 2,col = "darkgray")
#histogram of the residuals
hist(residual,prob=T,breaks=20,main="HISTOGRAM OF ACCELERATION RESIDUALS",xlab="Residuals")
lines(density(residual),col="red",lwd=3)
plot(acc_model)
# Make predictions and compute the R2, RMSE and MAE
acc_predict <- acc_model %>% predict(df_test)
data.frame( R2 = R2(acc_predict, df_test$mpg),
RMSE = RMSE(acc_predict, df_test$mpg),
MAE = MAE(acc_predict, df_test$mpg))
prediction_error = RMSE(acc_predict, df_test$mpg)/mean(df_test$mpg)
prediction_error
compare_acc = as.data.frame(cbind(df_test$mpg,acc_predict),row=FALSE)
names(compare_acc) = c("observed","acc_predict")
compare_acc
# To find out which independent variable to use in our multiple regression we are going to use the step wise regression
null=lm(mpg~1,data=df_train)
full=lm(mpg~.,data=df_train)
step(null,scope=list(upper=full),data=df_train,direction="both")
final_model <- lm(mpg ~ weight + horsepower, data = df_train)
summary(final_model)
plot(final_model)
#residuals vs. the predictor variable
residual <- final_model$residuals
plot(df_train$mpg~residual,lwd=3, col="blue",main="mpg vs residual", xlab="residual",ylab = "mpg")
grid(NA, 5, lwd = 2,col = "darkgray")
#absolute value of the residuals vs. the predictor variable
abs_residual <- abs(residual)
plot(df_train$mpg~abs_residual,lwd=3, col="red",main="mpg vs Abs_residual", xlab="abs_residual",ylab = "mpg")
grid(NA, 5, lwd = 2,col = "darkgray")
#histogram of the residuals
hist(residual,prob=T,breaks=20,main="HISTOGRAM OF weight + horsepower RESIDUALS",xlab="Residuals")
lines(density(residual),col="red",lwd=3)
# Global Validation of Linear Models Assumptions
gvlma(final_model)
final_model2 <- lm(mpg ~ weight + displacement, data = df_train[-c(112,245,248),])
summary(final_model2)
plot(final_model2)
# Global Validation of Linear Models Assumptions
gvlma(final_model2)
# Make predictions and compute the R2, RMSE and MAE
predict_final <- final_model %>% predict(df_test)
data.frame( R2 = R2(predict_final, df_test$mpg),
RMSE = RMSE(predict_final, df_test$mpg),
MAE = MAE(predict_final, df_test$mpg))
predictions_error <- RMSE(predict_final, df_test$mpg)/mean(df_test$mpg)
predictions_error
compare_final <- as.data.frame(cbind(df_test$mpg,predict_final),row=FALSE)
names(compare_final) <- c("observed","predict_final")
compare_final
cor(compare_final)
summary(compare_final)
plot(compare_final)
compare_final$error = compare_final$observed - compare_final$predict_final
# compare_final$residuals = final_model$residuals
compare_final
summary(compare_final)
plot(compare_final$error)
abline(h = 0, col= 'red')
length(final_model$residuals)
length(compare_final$error)
#histogram of the residuals
hist(compare_final$error,prob=T,breaks=20,main="HISTOGRAM OF weight + horsepower RESIDUALS",xlab="Residuals")
lines(density(compare_final$error),col="red",lwd=3)
final_model2 <- lm(mpg ~ weight + horsepower, data = df_train[-c(112,245,248),])
summary(final_model2)
plot(final_model2)
# Global Validation of Linear Models Assumptions
gvlma(final_model2)
# Make predictions and compute the R2, RMSE and MAE
predict_final <- final_model %>% predict(df_test)
data.frame( R2 = R2(predict_final, df_test$mpg),
RMSE = RMSE(predict_final, df_test$mpg),
MAE = MAE(predict_final, df_test$mpg))
predictions_error <- RMSE(predict_final, df_test$mpg)/mean(df_test$mpg)
predictions_error
compare_final <- as.data.frame(cbind(df_test$mpg,predict_final),row=FALSE)
names(compare_final) <- c("observed","predict_final")
compare_final
cor(compare_final)
summary(compare_final)
plot(compare_final)
compare_final$error = compare_final$observed - compare_final$predict_final
# compare_final$residuals = final_model$residuals
compare_final
summary(compare_final)
plot(compare_final$error)
abline(h = 0, col= 'red')
length(final_model$residuals)
length(compare_final$error)
#histogram of the residuals
hist(compare_final$error,prob=T,breaks=20,main="HISTOGRAM OF weight + horsepower RESIDUALS",xlab="Residuals")
lines(density(compare_final$error),col="red",lwd=3)
setwd("C:/Users/mosab/DS_Masters/DS_520/final_project")
#load necessary libraries
library(dplyr)
library(readxl)
library(ggplot2)
library(ggpubr)
library(qqplotr)
#library(car)
library(e1071)
library(nortest)
library(BSDA)
library(psych)
library(caret)
library(leaps)
library(gvlma)
file_name = "fidelity_mutual_funds_return_w_risk.csv"
df = read.csv(file_name)
df
str(df)
# Removing all case with null value in any columns
df <- na.omit(df)
summary(df)
resize.win <- function(Width=6, Height=6)
{
# works for windows
#dev.off(); # dev.new(width=6, height=6)
windows(record=TRUE, width=Width, height=Height)
}
resize.win(10,10)
# Scatter plot of matrices
dev.new(width=30, height=30)
pairs.panels(df[,2:18],method = "pearson",hist.col ="#00AFBB" ,density = TRUE,ellipses = TRUE )
# To find out which independent variable to use in our multiple regression we are going to use the step wise regression
null=lm(yr10~1,data=df[,3:18])
full=lm(yr10~.,data=df[,3:18])
step(null,scope=list(upper=full),data=df_train,direction="both")
fit = lm(formula = yr10 ~ yr5 + yr3 + life_of_fund + ytdDaily + yr1 +
r2 + gross_expense_ratio + morningstar_rating_overall + risk +
sharpe_ratio_3_yr + std_dev + net_expense_ratio + minimum_investment,
data = df[, 3:18])
summary(fit)
anova(fit)
# Global Validation of Linear Models Assumptions
gvlma(fit)
anova(fit)[3]
anova(fit)[4]
sum(anova(fit)[4])
sum(anova(fit)[4])
anova(fit)[4]
sum(anova(fit)[, 2])
sum(anova(fit)[, 4])
sum(anova(fit)[1:-1, 4])
sum(anova(fit)[, 4])
sum(anova(fit)[, 3])
sum(anova(fit)[1:13, 3])
sum(anova(fit)[1:13, 4])
sum(anova(fit)[2:13, 4])
